{
  "cells": [
    {
      "cell_type": "code",
      "id": "hh4MuZ6suSmFQiCpWjIX0yQ7",
      "metadata": {
        "tags": [],
        "id": "hh4MuZ6suSmFQiCpWjIX0yQ7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720622116458,
          "user_tz": -120,
          "elapsed": 56892,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "#Import different libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = 'pulmobucket_dataset'\n",
        "DATA_DIR = 'data/train'\n",
        "auth.authenticate_user()\n",
        "storage_client = storage.Client()"
      ],
      "metadata": {
        "id": "lN487qK8_zsR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720622147784,
          "user_tz": -120,
          "elapsed": 401,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "lN487qK8_zsR",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters setup\n",
        "IMG_HEIGHT, IMG_WIDTH = 150, 150\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "CLASS_NAMES = ['covid', 'pneumonia', 'normal', 'lung_opacity']"
      ],
      "metadata": {
        "id": "auJ-qI7lI51_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720622149136,
          "user_tz": -120,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "auJ-qI7lI51_",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storage_client = storage.Client()\n",
        "\n",
        "\n",
        "def list_gcs_files(bucket_name, prefix):\n",
        "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
        "    return [blob.name for blob in blobs if not blob.name.endswith('/')]\n",
        "\n",
        "def decode_img(img):\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    return img / 255.0\n",
        "\n",
        "def process_path(file_path):\n",
        "    parts = tf.strings.split(file_path, os.path.sep)\n",
        "    class_str = parts[-2]\n",
        "    label = tf.cast(tf.equal(class_str, CLASS_NAMES), tf.int32)\n",
        "    label = tf.argmax(label)\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = decode_img(img)\n",
        "    return img, label\n",
        "\n",
        "def get_gcs_dataset(bucket_name, data_dir, batch_size, img_height, img_width):\n",
        "    gcs_files = []\n",
        "    for class_name in CLASS_NAMES:\n",
        "        class_files = list_gcs_files(bucket_name, f'{data_dir}/{class_name}')\n",
        "        gcs_files.extend([f'gs://{bucket_name}/{file}' for file in class_files])\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(gcs_files)\n",
        "    dataset = dataset.map(lambda x: process_path(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    dataset_size = len(gcs_files)\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    val_size = dataset_size - train_size\n",
        "\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    val_dataset = dataset.skip(train_size)\n",
        "\n",
        "    train_dataset = train_dataset.cache().shuffle(buffer_size=train_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Obtenir les datasets d'entraînement et de\n",
        "print(\"loading data\")\n",
        "train_dataset, val_dataset = get_gcs_dataset(BUCKET_NAME, DATA_DIR, BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH)"
      ],
      "metadata": {
        "id": "n0hko4IXIs1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720622172791,
          "user_tz": -120,
          "elapsed": 21728,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5583d1cb-2f53-4c8b-8862-8df765cae6dc"
      },
      "id": "n0hko4IXIs1F",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir le modèle\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "5nprpJCA4SsY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720622329713,
          "user_tz": -120,
          "elapsed": 1295,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "5nprpJCA4SsY",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiler le modèle\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "C6sQiozk4WDq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720622332957,
          "user_tz": -120,
          "elapsed": 287,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "C6sQiozk4WDq",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"best_model.h5\"\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "Qd0255RR6WAm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720622334971,
          "user_tz": -120,
          "elapsed": 275,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "Qd0255RR6WAm",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraîner le modèle\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYZJ3LYD7Igt",
        "outputId": "53b659f9-6510-4d62-e23b-75154915e910",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720635525376,
          "user_tz": -120,
          "elapsed": 13187862,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "LYZJ3LYD7Igt",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "817/817 [==============================] - ETA: 0s - loss: 0.4816 - accuracy: 0.8416\n",
            "Epoch 1: val_loss improved from inf to 2.07879, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r817/817 [==============================] - 8039s 6s/step - loss: 0.4816 - accuracy: 0.8416 - val_loss: 2.0788 - val_accuracy: 0.2818\n",
            "Epoch 2/2\n",
            "817/817 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9246\n",
            "Epoch 2: val_loss improved from 2.07879 to 1.30719, saving model to best_model.h5\n",
            "817/817 [==============================] - 5123s 6s/step - loss: 0.2171 - accuracy: 0.9246 - val_loss: 1.3072 - val_accuracy: 0.4916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "def save_and_upload_model_to_gcs(model, bucket_name, destination_blob_name):\n",
        "    # Chemin temporaire pour sauvegarder le fichier pickle localement\n",
        "    local_path = '/tmp/model.pkl'\n",
        "\n",
        "    # Sauvegarder le modèle dans un fichier pickle\n",
        "    with open(local_path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Vérifiez que le fichier a été créé\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"Model saved locally at {local_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to save model locally at {local_path}\")\n",
        "        return\n",
        "\n",
        "    # Initialiser le client GCS\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "        # Télécharger le fichier pickle dans le bucket GCS\n",
        "        blob.upload_from_filename(local_path)\n",
        "        print(f'Model uploaded to {destination_blob_name} in bucket {bucket_name}')\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to upload model to GCS: {e}\")"
      ],
      "metadata": {
        "id": "0aKbABSN7WPM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720645794713,
          "user_tz": -120,
          "elapsed": 281,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "0aKbABSN7WPM",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'pulmobucket_models'\n",
        "destination_blob_name = 'GCPmodel.pkl'\n",
        "\n",
        "save_and_upload_model_to_gcs(model, bucket_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "xx5xkFKjuiU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1720646028797,
          "user_tz": -120,
          "elapsed": 55766,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "448c75ee-314e-4686-a348-7cd3cbdff284"
      },
      "id": "xx5xkFKjuiU-",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved locally at /tmp/model.pkl\n",
            "Model uploaded to GCPmodel.pkl in bucket pulmobucket_models\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "pulmo_notebook_train"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}